{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATEST - BBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LR1Parser:\n",
    "    def __init__(self):\n",
    "        self.terminals = set()\n",
    "        self.non_terminals = set()\n",
    "        self.productions = []\n",
    "        self.start_symbol = None\n",
    "        self.parsing_table = {}\n",
    "        self.canonical_collection = []\n",
    "        \n",
    "    def _compute_first_sets(self) -> Dict[Symbol, Set[Symbol]]:\n",
    "        first_sets = {sym: set() for sym in self.terminals.union(self.non_terminals)}\n",
    "        \n",
    "        # Terminals have themselves as first set\n",
    "        for t in self.terminals:\n",
    "            first_sets[t] = {t}\n",
    "        \n",
    "        # Compute first sets\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for prod in self.productions:\n",
    "                # Add first of first symbol in production's right side\n",
    "                if not prod.right:  # Epsilon production\n",
    "                    if Symbol('ε', True) not in first_sets[prod.left]:\n",
    "                        first_sets[prod.left].add(Symbol('ε', True))\n",
    "                        changed = True\n",
    "                elif prod.right[0].is_terminal:\n",
    "                    if prod.right[0] not in first_sets[prod.left]:\n",
    "                        first_sets[prod.left].add(prod.right[0])\n",
    "                        changed = True\n",
    "                else:\n",
    "                    # For non-terminals, add their first sets\n",
    "                    first_of_first = first_sets[prod.right[0]]\n",
    "                    for first_sym in first_of_first:\n",
    "                        if first_sym not in first_sets[prod.left]:\n",
    "                            first_sets[prod.left].add(first_sym)\n",
    "                            changed = True\n",
    "        \n",
    "        return first_sets\n",
    "\n",
    "    def _compute_follow_sets(self, first_sets: Dict[Symbol, Set[Symbol]]) -> Dict[Symbol, Set[Symbol]]:\n",
    "        follow_sets = {nt: set() for nt in self.non_terminals}\n",
    "        \n",
    "        # Start symbol has end marker\n",
    "        follow_sets[self.start_symbol].add(Symbol('$', True))\n",
    "        \n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for prod in self.productions:\n",
    "                for i, sym in enumerate(prod.right):\n",
    "                    if not sym.is_terminal:\n",
    "                        # Find what can follow this non-terminal\n",
    "                        if i == len(prod.right) - 1:\n",
    "                            # If at end of production, add follow of left side\n",
    "                            for follow_sym in follow_sets[prod.left]:\n",
    "                                if follow_sym not in follow_sets[sym]:\n",
    "                                    follow_sets[sym].add(follow_sym)\n",
    "                                    changed = True\n",
    "                        else:\n",
    "                            # Add first set of next symbol\n",
    "                            next_sym = prod.right[i+1]\n",
    "                            if next_sym.is_terminal:\n",
    "                                if next_sym not in follow_sets[sym]:\n",
    "                                    follow_sets[sym].add(next_sym)\n",
    "                                    changed = True\n",
    "                            else:\n",
    "                                # Add first set of next non-terminal\n",
    "                                for first_sym in first_sets[next_sym]:\n",
    "                                    if first_sym.name != 'ε' and first_sym not in follow_sets[sym]:\n",
    "                                        follow_sets[sym].add(first_sym)\n",
    "                                        changed = True\n",
    "        \n",
    "        return follow_sets\n",
    "\n",
    "    def _create_json_grammar(self):\n",
    "        # Define terminals\n",
    "        self.terminals = {\n",
    "            Symbol('{', True), Symbol('}', True), Symbol('[', True), Symbol(']', True),\n",
    "            Symbol(':', True), Symbol(',', True), \n",
    "            Symbol('string', True), Symbol('number', True), \n",
    "            Symbol('true', True), Symbol('false', True), Symbol('null', True)\n",
    "        }\n",
    "\n",
    "        # Define non-terminals\n",
    "        self.non_terminals = {\n",
    "            Symbol('JSON', False), Symbol('Object', False), Symbol('Array', False),\n",
    "            Symbol('Pair', False), Symbol('Value', False), Symbol('Elements', False)\n",
    "        }\n",
    "\n",
    "        # Set start symbol\n",
    "        self.start_symbol = Symbol('JSON', False)\n",
    "\n",
    "        # Define productions\n",
    "        self.productions = [\n",
    "            Production(Symbol('JSON', False), [Symbol('Object', False)]),\n",
    "            Production(Symbol('JSON', False), [Symbol('Array', False)]),\n",
    "            \n",
    "            Production(Symbol('Object', False), [Symbol('{', True), Symbol('Pair', False), Symbol('}', True)]),\n",
    "            Production(Symbol('Object', False), [Symbol('{', True), Symbol('Pair', False), Symbol(',', True), Symbol('Pair', False), Symbol('}', True)]),\n",
    "            \n",
    "            Production(Symbol('Pair', False), [Symbol('string', True), Symbol(':', True), Symbol('Value', False)]),\n",
    "            Production(Symbol('Pair', False), [Symbol('Pair', False), Symbol(',', True), Symbol('string', True), Symbol(':', True), Symbol('Value', False)]),\n",
    "            \n",
    "            Production(Symbol('Array', False), [Symbol('[', True), Symbol('Value', False), Symbol(']', True)]),            \n",
    "            Production(Symbol('Array', False), [Symbol('[', True), Symbol('Value', False), Symbol(',', True), Symbol('Value', False), Symbol(']', True)]),\n",
    "            \n",
    "            Production(Symbol('Elements', False), [Symbol('Value', False)]),\n",
    "            Production(Symbol('Elements', False), [Symbol('Elements', False), Symbol(',', True), Symbol('Value', False)]),\n",
    "            \n",
    "            Production(Symbol('Value', False), [Symbol('string', True)]),\n",
    "            Production(Symbol('Value', False), [Symbol('number', True)]),\n",
    "            Production(Symbol('Value', False), [Symbol('Object', False)]),\n",
    "            Production(Symbol('Value', False), [Symbol('Array', False)]),\n",
    "            Production(Symbol('Value', False), [Symbol('true', True)]),\n",
    "            Production(Symbol('Value', False), [Symbol('false', True)]),\n",
    "            Production(Symbol('Value', False), [Symbol('null', True)])\n",
    "        ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.terminals = set()\n",
    "        self.non_terminals = set()\n",
    "        self.productions = []\n",
    "        self.start_symbol = None\n",
    "        self.parsing_table = {}\n",
    "        self.canonical_collection = []\n",
    "        \n",
    "    def _closure(self, items: Set[LRItem], first_sets: Dict[Symbol, Set[Symbol]]) -> Set[LRItem]:\n",
    "        closure = set(items)\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for item in list(closure):\n",
    "                # If dot is before a non-terminal\n",
    "                if item.dot_position < len(item.production.right) and not item.production.right[item.dot_position].is_terminal:\n",
    "                    sym = item.production.right[item.dot_position]\n",
    "                    beta_first = set()\n",
    "\n",
    "                    # Compute FIRST(β lookahead) if there's more in the production\n",
    "                    if item.dot_position + 1 < len(item.production.right):\n",
    "                        beta = item.production.right[item.dot_position + 1:]\n",
    "                        for b_sym in beta:\n",
    "                            beta_first.update(first_sets.get(b_sym, set()))\n",
    "                            if Symbol('ε', True) not in first_sets[b_sym]:\n",
    "                                break\n",
    "                    else:\n",
    "                        beta_first.add(item.lookahead)\n",
    "\n",
    "                    # Add new LR items\n",
    "                    lookaheads = beta_first or {item.lookahead}\n",
    "                    for prod in [p for p in self.productions if p.left == sym]:\n",
    "                        for lookahead in lookaheads:\n",
    "                            new_item = LRItem(prod, 0, lookahead)\n",
    "                            if new_item not in closure:\n",
    "                                closure.add(new_item)\n",
    "                                changed = True\n",
    "        return closure\n",
    "\n",
    "\n",
    "    def _goto(self, items: Set[LRItem], symbol: Symbol, first_sets: Dict[Symbol, Set[Symbol]]) -> Set[LRItem]:\n",
    "        goto_items = set()\n",
    "        \n",
    "        for item in items:\n",
    "            # If dot is before the given symbol\n",
    "            if (item.dot_position < len(item.production.right) and \n",
    "                item.production.right[item.dot_position] == symbol):\n",
    "                # Create new item with dot moved\n",
    "                new_item = LRItem(item.production, item.dot_position + 1, item.lookahead)\n",
    "                goto_items.add(new_item)\n",
    "        \n",
    "        return self._closure(goto_items, first_sets)\n",
    "\n",
    "    def _build_canonical_collection(self):\n",
    "        first_sets = self._compute_first_sets()\n",
    "        \n",
    "        # Start with initial item\n",
    "        initial_production = [p for p in self.productions if p.left == self.start_symbol][0]\n",
    "        initial_item = LRItem(initial_production, 0, Symbol('$', True))\n",
    "        initial_state = self._closure({initial_item}, first_sets)\n",
    "\n",
    "        \n",
    "        self.canonical_collection = [initial_state]\n",
    "        work_list = [initial_state]\n",
    "        \n",
    "        while work_list:\n",
    "            current_state = work_list.pop(0)\n",
    "            \n",
    "            # Find possible transitions\n",
    "            symbols = set()\n",
    "            for item in current_state:\n",
    "                if item.dot_position < len(item.production.right):\n",
    "                    symbols.add(item.production.right[item.dot_position])\n",
    "            \n",
    "            for symbol in symbols:\n",
    "                goto_state = self._goto(current_state, symbol, first_sets)\n",
    "                \n",
    "                # Check if this state already exists\n",
    "                if goto_state and goto_state not in self.canonical_collection:\n",
    "                    self.canonical_collection.append(goto_state)\n",
    "                    work_list.append(goto_state)\n",
    "\n",
    "    def _build_parsing_table(self):\n",
    "        self._build_canonical_collection()\n",
    "        first_sets = self._compute_first_sets()\n",
    "        follow_sets = self._compute_follow_sets(first_sets)\n",
    "\n",
    "        # Initialize parsing table\n",
    "        self.parsing_table = {}\n",
    "        for state_index, state in enumerate(self.canonical_collection):\n",
    "            self.parsing_table[state_index] = {}\n",
    "            for item in state:\n",
    "                if item.dot_position == len(item.production.right):  # Reduction\n",
    "                    if item.production.left == self.start_symbol:  # Accept action\n",
    "                        self.parsing_table[state_index][Symbol('$', True)] = ('accept', None)\n",
    "                    else:\n",
    "                        # Use Follow Set for reductions\n",
    "                        for follow in follow_sets[item.production.left]:\n",
    "                            if follow not in self.parsing_table[state_index]:\n",
    "                                self.parsing_table[state_index][follow] = ('reduce', item.production)\n",
    "                elif item.dot_position < len(item.production.right):  # Shift\n",
    "                    symbol = item.production.right[item.dot_position]\n",
    "                    goto_state = self._goto(state, symbol, first_sets)\n",
    "                    if goto_state in self.canonical_collection:\n",
    "                        goto_index = self.canonical_collection.index(goto_state)\n",
    "                        self.parsing_table[state_index][symbol] = ('shift', goto_index)\n",
    "\n",
    "\n",
    "    def parse(self, tokens):\n",
    "        # Ensure parsing table is built\n",
    "        if not self.parsing_table:\n",
    "            self._build_parsing_table()\n",
    "        \n",
    "        # Convert tokens to symbols\n",
    "        token_symbols = [Symbol(token[1], True) for token in tokens]\n",
    "        token_symbols.append(Symbol('$', True))\n",
    "        \n",
    "        # Initialize parse stack\n",
    "        stack = [0]  # Start with initial state\n",
    "        input_ptr = 0\n",
    "        \n",
    "        while True:\n",
    "            current_state = stack[-1]\n",
    "            current_token = token_symbols[input_ptr]\n",
    "            \n",
    "            print(f\"Current State: {current_state}, Current Token: {current_token}\")\n",
    "            print(f\"Parsing Action: {self.parsing_table.get(current_state, {}).get(current_token, 'Error')}\")\n",
    "            \n",
    "            # Look up action in parsing table\n",
    "            if current_token not in self.parsing_table[current_state]:\n",
    "                # Error handling\n",
    "                raise ValueError(f\"Syntax error: Unexpected token {current_token}\")\n",
    "            \n",
    "            action, details = self.parsing_table[current_state][current_token]\n",
    "            \n",
    "            if action == 'shift':\n",
    "                stack.append(current_token)\n",
    "                stack.append(details)  # Push state\n",
    "                input_ptr += 1\n",
    "            elif action == 'reduce':\n",
    "                # Pop states and symbols\n",
    "                pop_count = len(details.right) * 2\n",
    "                stack = stack[:-pop_count]\n",
    "                \n",
    "                # Push non-terminal and go to goto state\n",
    "                stack.append(details.left)\n",
    "                goto_state = self.parsing_table[stack[-2]][details.left][1]\n",
    "                stack.append(goto_state)\n",
    "            elif action == 'accept':\n",
    "                return True  # Parsing successful\n",
    "            \n",
    "    def visualize_parse_tree(self, tokens):\n",
    "        \"\"\"\n",
    "        Visualize the parse tree for a given input with more accurate parsing.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[Tuple]): List of parsed tokens\n",
    "\n",
    "        Returns:\n",
    "            graphviz.Digraph: A visual representation of the parse tree\n",
    "        \"\"\"\n",
    "        dot = graphviz.Digraph(comment='Parse Tree', format='png')\n",
    "        dot.attr(rankdir='TB')  # Top to Bottom layout\n",
    "\n",
    "        # Track parse tree construction during parsing\n",
    "        \n",
    "\n",
    "        def parse_with_tree_construction():\n",
    "            stack = [0]  # Start with initial state\n",
    "            input_ptr = 0\n",
    "            token_symbols = [Symbol(token[1], True) for token in tokens]\n",
    "            token_symbols.append(Symbol('$', True))\n",
    "\n",
    "            # Track current production hierarchy\n",
    "            production_stack = []\n",
    "            parse_tree_nodes = []\n",
    "            while True:\n",
    "                current_state = stack[-1]\n",
    "                current_token = token_symbols[input_ptr]\n",
    "\n",
    "                if current_token not in self.parsing_table[current_state]:\n",
    "                    raise ValueError(f\"Syntax error: Unexpected token {current_token}\")\n",
    "\n",
    "                action, details = self.parsing_table[current_state][current_token]\n",
    "\n",
    "                if action == 'shift':\n",
    "                    # Create node for terminal\n",
    "                    node_id = f\"token_{input_ptr}\"\n",
    "                    dot.node(node_id, current_token.name, shape='ellipse', style='filled', fillcolor='lightgreen')\n",
    "\n",
    "                    # Connect to parent if exists\n",
    "                    if production_stack:\n",
    "                        dot.edge(str(id(production_stack[-1])), node_id)\n",
    "\n",
    "                    # Add to tracking\n",
    "                    parse_tree_nodes.append((node_id, current_token))\n",
    "\n",
    "                    stack.append(current_token)\n",
    "                    stack.append(details)\n",
    "                    input_ptr += 1\n",
    "\n",
    "                elif action == 'reduce':\n",
    "                    # Create node for production\n",
    "                    node_id = str(id(details))\n",
    "                    dot.node(node_id, details.left.name, shape='box', style='filled', fillcolor='lightyellow')\n",
    "\n",
    "                    # Connect children\n",
    "                    child_count = len(details.right)\n",
    "                    children = parse_tree_nodes[-child_count:]\n",
    "                    for child_node_id, _ in children:\n",
    "                        dot.edge(node_id, child_node_id)\n",
    "\n",
    "                    # Remove used children\n",
    "                    parse_tree_nodes = parse_tree_nodes[:-child_count]\n",
    "                    parse_tree_nodes.append((node_id, details.left))\n",
    "\n",
    "                    # Manage production stack for hierarchy\n",
    "                    production_stack.append(details)\n",
    "\n",
    "                    # Pop states and symbols\n",
    "                    pop_count = len(details.right) * 2\n",
    "                    stack = stack[:-pop_count]\n",
    "\n",
    "                    # Push non-terminal and go to goto state\n",
    "                    stack.append(details.left)\n",
    "                    goto_state = self.parsing_table[stack[-2]][details.left][1]\n",
    "                    stack.append(goto_state)\n",
    "\n",
    "                elif action == 'accept':\n",
    "                    break\n",
    "                \n",
    "        # Run parsing with tree construction\n",
    "        parse_with_tree_construction()\n",
    "\n",
    "        # Render the graph\n",
    "        dot.render('parse_tree', view=True)\n",
    "        return dot\n",
    "    \n",
    "    def get_intermediate_representation(self, tokens):\n",
    "        \"\"\"\n",
    "        Generate an intermediate representation of the parsing process.\n",
    "\n",
    "        Args:\n",
    "            tokens (List[Tuple]): List of parsed tokens\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary representing the parsing stages and intermediate structures\n",
    "        \"\"\"\n",
    "        intermediate_repr = {\n",
    "            'tokens': tokens,\n",
    "            'parsing_stages': [],\n",
    "            'final_ast': None\n",
    "        }\n",
    "\n",
    "        def create_ast_node(production, children=None):\n",
    "            \"\"\"Create an abstract syntax tree node\"\"\"\n",
    "            return {\n",
    "                'type': production.left.name,\n",
    "                'children': children or []\n",
    "            }\n",
    "\n",
    "        def parse_with_stages():\n",
    "            stack = [0]  # Start with initial state\n",
    "            input_ptr = 0\n",
    "            token_symbols = [Symbol(token[1], True) for token in tokens]\n",
    "            token_symbols.append(Symbol('$', True))\n",
    "\n",
    "            current_ast = None\n",
    "\n",
    "            while True:\n",
    "                current_state = stack[-1]\n",
    "                current_token = token_symbols[input_ptr]\n",
    "\n",
    "                # Capture current parsing stage\n",
    "                intermediate_repr['parsing_stages'].append({\n",
    "                    'state': current_state,\n",
    "                    'token': current_token,\n",
    "                    'stack': stack.copy(),\n",
    "                    'input_ptr': input_ptr\n",
    "                })\n",
    "\n",
    "                if current_token not in self.parsing_table[current_state]:\n",
    "                    raise ValueError(f\"Syntax error: Unexpected token {current_token}\")\n",
    "\n",
    "                action, details = self.parsing_table[current_state][current_token]\n",
    "\n",
    "                if action == 'shift':\n",
    "                    stack.append(current_token)\n",
    "                    stack.append(details)\n",
    "                    input_ptr += 1\n",
    "                elif action == 'reduce':\n",
    "                    # Create AST node for reduction\n",
    "                    child_nodes = []\n",
    "                    pop_count = len(details.right) * 2\n",
    "                    stack = stack[:-pop_count]\n",
    "\n",
    "                    stack.append(details.left)\n",
    "                    current_ast = create_ast_node(details)\n",
    "                    goto_state = self.parsing_table[stack[-2]][details.left][1]\n",
    "                    stack.append(goto_state)\n",
    "                elif action == 'accept':\n",
    "                    intermediate_repr['final_ast'] = current_ast\n",
    "                    break\n",
    "                \n",
    "        parse_with_stages()\n",
    "        return intermediate_repr\n",
    "    \n",
    "\n",
    "def tokenize_json(json_string):\n",
    "    # Implement JSON tokenization\n",
    "    token_patterns = [\n",
    "        (r'\\s+', None),  # Whitespace\n",
    "        (r'\\{', '{'),\n",
    "        (r'\\}', '}'),\n",
    "        (r'\\[', '['),\n",
    "        (r'\\]', ']'),\n",
    "        (r':', ':'),\n",
    "        (r',', ','),\n",
    "        (r'\"(?:\\\\.|[^\"\\\\])*\"', 'string'),  # Handle escaped quotes\n",
    "        (r'-?\\d+(\\.\\d+)?([eE][+-]?\\d+)?', 'number'),\n",
    "        (r'true', 'true'),\n",
    "        (r'false', 'false'),\n",
    "        (r'null', 'null')\n",
    "    ]\n",
    "    tokens = []\n",
    "    while json_string:\n",
    "        match = None\n",
    "        for pattern, token_type in token_patterns:\n",
    "            regex = re.compile(pattern)\n",
    "            match = regex.match(json_string)\n",
    "            if match:\n",
    "                text = match.group(0)\n",
    "                if token_type:\n",
    "                    tokens.append((text, token_type))\n",
    "                json_string = json_string[len(text):].lstrip()\n",
    "                break\n",
    "        \n",
    "        if not match:\n",
    "            raise ValueError(f\"Unexpected token in JSON: {json_string}\")\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [('{', '{'), ('\"name\"', 'string'), (':', ':'), ('\"John Doe\"', 'string'), (',', ','), ('\"age\"', 'string'), (':', ':'), ('30', 'number'), (',', ','), ('\"cities\"', 'string'), (':', ':'), ('[', '['), ('\"New York\"', 'string'), (',', ','), ('\"London\"', 'string'), (']', ']'), (',', ','), ('\"address\"', 'string'), (':', ':'), ('{', '{'), ('\"street\"', 'string'), (':', ':'), ('\"123 Main St\"', 'string'), (',', ','), ('\"city\"', 'string'), (':', ':'), ('\"Boston\"', 'string'), (',', ','), ('\"country\"', 'string'), (':', ':'), ('\"USA\"', 'string'), ('}', '}'), ('}', '}')]\n",
      "Current State: 0, Current Token: Terminal({)\n",
      "Parsing Action: ('shift', 1)\n",
      "Current State: 1, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 3)\n",
      "Current State: 3, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 5)\n",
      "Current State: 5, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 10)\n",
      "Current State: 10, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Value → string)\n",
      "Current State: 12, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Pair → string : Value)\n",
      "Current State: 4, Current Token: Terminal(,)\n",
      "Parsing Action: ('shift', 7)\n",
      "Current State: 7, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 18)\n",
      "Current State: 18, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 31)\n",
      "Current State: 31, Current Token: Terminal(number)\n",
      "Parsing Action: ('shift', 11)\n",
      "Current State: 11, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Value → number)\n",
      "Current State: 40, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Pair → string : Value)\n",
      "Current State: 19, Current Token: Terminal(,)\n",
      "Parsing Action: ('shift', 33)\n",
      "Current State: 33, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 41)\n",
      "Current State: 41, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 57)\n",
      "Current State: 57, Current Token: Terminal([)\n",
      "Parsing Action: ('shift', 14)\n",
      "Current State: 14, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 23)\n",
      "Current State: 23, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Value → string)\n",
      "Current State: 24, Current Token: Terminal(,)\n",
      "Parsing Action: ('shift', 34)\n",
      "Current State: 34, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 44)\n",
      "Current State: 44, Current Token: Terminal(])\n",
      "Parsing Action: ('reduce', Value → string)\n",
      "Current State: 46, Current Token: Terminal(])\n",
      "Parsing Action: ('shift', 58)\n",
      "Current State: 58, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Array → [ Value , Value ])\n",
      "Current State: 9, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Value → Array)\n",
      "Current State: 64, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Pair → Pair , string : Value)\n",
      "Current State: 19, Current Token: Terminal(,)\n",
      "Parsing Action: ('shift', 33)\n",
      "Current State: 33, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 41)\n",
      "Current State: 41, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 57)\n",
      "Current State: 57, Current Token: Terminal({)\n",
      "Parsing Action: ('shift', 15)\n",
      "Current State: 15, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 3)\n",
      "Current State: 3, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 5)\n",
      "Current State: 5, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 10)\n",
      "Current State: 10, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Value → string)\n",
      "Current State: 12, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Pair → string : Value)\n",
      "Current State: 30, Current Token: Terminal(,)\n",
      "Parsing Action: ('shift', 39)\n",
      "Current State: 39, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 18)\n",
      "Current State: 18, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 31)\n",
      "Current State: 31, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 10)\n",
      "Current State: 10, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Value → string)\n",
      "Current State: 40, Current Token: Terminal(,)\n",
      "Parsing Action: ('reduce', Pair → string : Value)\n",
      "Current State: 56, Current Token: Terminal(,)\n",
      "Parsing Action: ('shift', 33)\n",
      "Current State: 33, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 41)\n",
      "Current State: 41, Current Token: Terminal(:)\n",
      "Parsing Action: ('shift', 57)\n",
      "Current State: 57, Current Token: Terminal(string)\n",
      "Parsing Action: ('shift', 10)\n",
      "Current State: 10, Current Token: Terminal(})\n",
      "Parsing Action: ('reduce', Value → string)\n",
      "Current State: 64, Current Token: Terminal(})\n",
      "Parsing Action: ('reduce', Pair → Pair , string : Value)\n",
      "Current State: 56, Current Token: Terminal(})\n",
      "Parsing Action: ('shift', 63)\n",
      "Current State: 63, Current Token: Terminal(})\n",
      "Parsing Action: ('reduce', Object → { Pair , Pair })\n",
      "Current State: 17, Current Token: Terminal(})\n",
      "Parsing Action: ('reduce', Value → Object)\n",
      "Current State: 64, Current Token: Terminal(})\n",
      "Parsing Action: ('reduce', Pair → Pair , string : Value)\n",
      "Current State: 19, Current Token: Terminal(})\n",
      "Parsing Action: ('shift', 32)\n",
      "Current State: 32, Current Token: Terminal($)\n",
      "Parsing Action: ('reduce', Object → { Pair , Pair })\n",
      "Current State: 2, Current Token: Terminal($)\n",
      "Parsing Action: ('accept', None)\n",
      "Result: True \n",
      "\n",
      "Parsing successful!\n",
      "\n",
      "Intermediate Representation:\n",
      "\n",
      " {'tokens': [('{', '{'), ('\"name\"', 'string'), (':', ':'), ('\"John Doe\"', 'string'), (',', ','), ('\"age\"', 'string'), (':', ':'), ('30', 'number'), (',', ','), ('\"cities\"', 'string'), (':', ':'), ('[', '['), ('\"New York\"', 'string'), (',', ','), ('\"London\"', 'string'), (']', ']'), (',', ','), ('\"address\"', 'string'), (':', ':'), ('{', '{'), ('\"street\"', 'string'), (':', ':'), ('\"123 Main St\"', 'string'), (',', ','), ('\"city\"', 'string'), (':', ':'), ('\"Boston\"', 'string'), (',', ','), ('\"country\"', 'string'), (':', ':'), ('\"USA\"', 'string'), ('}', '}'), ('}', '}')], 'parsing_stages': [{'state': 0, 'token': Terminal({), 'stack': [0], 'input_ptr': 0}, {'state': 1, 'token': Terminal(string), 'stack': [0, Terminal({), 1], 'input_ptr': 1}, {'state': 3, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Terminal(string), 3], 'input_ptr': 2}, {'state': 5, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Terminal(string), 3, Terminal(:), 5], 'input_ptr': 3}, {'state': 10, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Terminal(string), 3, Terminal(:), 5, Terminal(string), 10], 'input_ptr': 4}, {'state': 12, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Terminal(string), 3, Terminal(:), 5, Non-Terminal(Value), 12], 'input_ptr': 4}, {'state': 4, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4], 'input_ptr': 4}, {'state': 7, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7], 'input_ptr': 5}, {'state': 18, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Terminal(string), 18], 'input_ptr': 6}, {'state': 31, 'token': Terminal(number), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Terminal(string), 18, Terminal(:), 31], 'input_ptr': 7}, {'state': 11, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Terminal(string), 18, Terminal(:), 31, Terminal(number), 11], 'input_ptr': 8}, {'state': 40, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Terminal(string), 18, Terminal(:), 31, Non-Terminal(Value), 40], 'input_ptr': 8}, {'state': 19, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19], 'input_ptr': 8}, {'state': 33, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33], 'input_ptr': 9}, {'state': 41, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41], 'input_ptr': 10}, {'state': 57, 'token': Terminal([), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57], 'input_ptr': 11}, {'state': 14, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14], 'input_ptr': 12}, {'state': 23, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14, Terminal(string), 23], 'input_ptr': 13}, {'state': 24, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14, Non-Terminal(Value), 24], 'input_ptr': 13}, {'state': 34, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14, Non-Terminal(Value), 24, Terminal(,), 34], 'input_ptr': 14}, {'state': 44, 'token': Terminal(]), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14, Non-Terminal(Value), 24, Terminal(,), 34, Terminal(string), 44], 'input_ptr': 15}, {'state': 46, 'token': Terminal(]), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14, Non-Terminal(Value), 24, Terminal(,), 34, Non-Terminal(Value), 46], 'input_ptr': 15}, {'state': 58, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal([), 14, Non-Terminal(Value), 24, Terminal(,), 34, Non-Terminal(Value), 46, Terminal(]), 58], 'input_ptr': 16}, {'state': 9, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Non-Terminal(Array), 9], 'input_ptr': 16}, {'state': 64, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Non-Terminal(Value), 64], 'input_ptr': 16}, {'state': 19, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19], 'input_ptr': 16}, {'state': 33, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33], 'input_ptr': 17}, {'state': 41, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41], 'input_ptr': 18}, {'state': 57, 'token': Terminal({), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57], 'input_ptr': 19}, {'state': 15, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15], 'input_ptr': 20}, {'state': 3, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Terminal(string), 3], 'input_ptr': 21}, {'state': 5, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Terminal(string), 3, Terminal(:), 5], 'input_ptr': 22}, {'state': 10, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Terminal(string), 3, Terminal(:), 5, Terminal(string), 10], 'input_ptr': 23}, {'state': 12, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Terminal(string), 3, Terminal(:), 5, Non-Terminal(Value), 12], 'input_ptr': 23}, {'state': 30, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30], 'input_ptr': 23}, {'state': 39, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39], 'input_ptr': 24}, {'state': 18, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Terminal(string), 18], 'input_ptr': 25}, {'state': 31, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Terminal(string), 18, Terminal(:), 31], 'input_ptr': 26}, {'state': 10, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Terminal(string), 18, Terminal(:), 31, Terminal(string), 10], 'input_ptr': 27}, {'state': 40, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Terminal(string), 18, Terminal(:), 31, Non-Terminal(Value), 40], 'input_ptr': 27}, {'state': 56, 'token': Terminal(,), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56], 'input_ptr': 27}, {'state': 33, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56, Terminal(,), 33], 'input_ptr': 28}, {'state': 41, 'token': Terminal(:), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56, Terminal(,), 33, Terminal(string), 41], 'input_ptr': 29}, {'state': 57, 'token': Terminal(string), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57], 'input_ptr': 30}, {'state': 10, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal(string), 10], 'input_ptr': 31}, {'state': 64, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Non-Terminal(Value), 64], 'input_ptr': 31}, {'state': 56, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56], 'input_ptr': 31}, {'state': 63, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Terminal({), 15, Non-Terminal(Pair), 30, Terminal(,), 39, Non-Terminal(Pair), 56, Terminal(}), 63], 'input_ptr': 32}, {'state': 17, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Non-Terminal(Object), 17], 'input_ptr': 32}, {'state': 64, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(,), 33, Terminal(string), 41, Terminal(:), 57, Non-Terminal(Value), 64], 'input_ptr': 32}, {'state': 19, 'token': Terminal(}), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19], 'input_ptr': 32}, {'state': 32, 'token': Terminal($), 'stack': [0, Terminal({), 1, Non-Terminal(Pair), 4, Terminal(,), 7, Non-Terminal(Pair), 19, Terminal(}), 32], 'input_ptr': 33}, {'state': 2, 'token': Terminal($), 'stack': [0, Non-Terminal(Object), 2], 'input_ptr': 33}], 'final_ast': {'type': 'Object', 'children': []}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    parser = LR1Parser()\n",
    "    parser._create_json_grammar()\n",
    "    with open('test.json', 'r', encoding='utf-8') as f:\n",
    "        json_str = f.read()\n",
    "\n",
    "    tokens = tokenize_json(json_str)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "    try:\n",
    "        result = parser.parse(tokens)\n",
    "        print(\"Result:\", result,\"\\n\")\n",
    "        print(\"Parsing successful!\\n\")\n",
    "        parser.visualize_parse_tree(tokens)\n",
    "        ir = parser.get_intermediate_representation(tokens) \n",
    "        print(\"Intermediate Representation:\\n\\n\", ir)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Parsing error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
